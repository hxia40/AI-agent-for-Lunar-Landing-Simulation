{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from HX_DieN import DieNEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()  # Resetting the environment will return an integer. This number will be our initial state.\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "\n",
    "        # total_reward += (gamma ** step_idx * reward)\n",
    "        # the above code is from Moustafa Alzantot , which this is problematic.\n",
    "        # As the policy's target here is never to finish in shortest time. Rather,\n",
    "        # the only thing matters is that if u can successfully recover ur stuff, or drop into one of the ice-hole.\n",
    "        total_reward += reward    # HX\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    # print \"total_reward:\", total_reward\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 1000):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    for _ in range(n):\n",
    "        if _ % (n/5) == 0:\n",
    "            print(\"=====sample went through =====\", _)\n",
    "        scores.append(run_episode(env, policy, gamma = gamma, render = False))\n",
    "\n",
    "\n",
    "    # # scores = [\n",
    "    # #         run_episode(env, policy, gamma = gamma, render = False)\n",
    "    # #         for _ in range(n)]\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"time consumed is\",end_time)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def run_episode_stock(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()  # Resetting the environment will return an integer. This number will be our initial state.\n",
    "    obs = 11  # that's what usually happens. for PI/VI for the daily trading, obs has to be set to 0\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        actual_obs_not_using, reward, done , _ = env.step(int(policy[obs]))\n",
    "        obs += 1\n",
    "        # print('obs:', obs, 'r:', reward, 'done:', done)\n",
    "        # time.sleep(0.3)\n",
    "\n",
    "        # total_reward += (gamma ** step_idx * reward)\n",
    "        # the above code is from Moustafa Alzantot , which this is problematic.\n",
    "        # As the policy's target here is never to finish in shortest time. Rather,\n",
    "        # the only thing matters is that if u can successfully recover ur stuff, or drop into one of the ice-hole.\n",
    "        total_reward += reward    # HX\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "\n",
    "            break\n",
    "    # print \"total_reward:\", total_reward\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy_stock(env, policy, gamma = 1.0,  n = 1000):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode_stock(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "class VI:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.V = np.zeros(env.nS)\n",
    "\n",
    "    def next_best_action(self, s, V, gamma=1):\n",
    "        # print(\"\\ns =\" , s)\n",
    "        action_values = np.zeros(self.env.nA)\n",
    "        for a in range(self.env.nA):\n",
    "            for prob, next_state, reward, done in self.env.P[s][a]:\n",
    "                # print('prob:', prob, 's_:', next_state, 'r:', reward, 'done:', done)\n",
    "                action_values[a] += prob * (reward + gamma * V[next_state])\n",
    "        return np.argmax(action_values), np.max(action_values)\n",
    "\n",
    "    def optimize(self, gamma =1):\n",
    "        # THETA = 12.3636363637\n",
    "        delta = float(\"inf\")\n",
    "        last_delta = float(\"inf\")\n",
    "        round_num = 0\n",
    "\n",
    "        while delta:\n",
    "            start_time = time.time()\n",
    "            delta = 0\n",
    "            # print(\"\\nValue Iteration: Round \" + str(round_num))\n",
    "            # print(np.reshape(self.V,(8,8)))\n",
    "            for s in range(self.env.nS):\n",
    "                best_action, best_action_value = self.next_best_action(s, self.V, gamma)\n",
    "                delta = max(delta, np.abs(best_action_value - self.V[s]))\n",
    "                self.V[s] = best_action_value\n",
    "            if last_delta/delta < 1.0000000000001:\n",
    "                break\n",
    "            else:\n",
    "                round_num += 1\n",
    "                last_delta = delta\n",
    "#                 print('round_num/delta/time:', round_num, delta, time.time()-start_time)\n",
    "\n",
    "        policy = np.zeros(self.env.nS)\n",
    "        for s in range(self.env.nS):\n",
    "            best_action, best_action_value = self.next_best_action(s, self.V, gamma)\n",
    "            policy[s] = best_action\n",
    "#         print('VI policy:', policy)\n",
    "#         print('VI table:', self.V)\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====innerloop: 23 [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] =========\n",
      "=====sample went through ===== 0\n",
      "=====sample went through ===== 12000000\n",
      "=====sample went through ===== 24000000\n",
      "=====sample went through ===== 36000000\n",
      "=====sample went through ===== 48000000\n",
      "time consumed is 966.464302778244\n",
      "Policy average score =  8.55319345\n",
      "====innerloop: 2 [0, 1] =========\n",
      "=====sample went through ===== 0\n",
      "=====sample went through ===== 12000000\n",
      "=====sample went through ===== 24000000\n",
      "=====sample went through ===== 36000000\n",
      "=====sample went through ===== 48000000\n",
      "time consumed is 696.6600723266602\n",
      "Policy average score =  0.500020116667\n",
      "====innerloop: 28 [0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1] =========\n",
      "=====sample went through ===== 0\n",
      "=====sample went through ===== 12000000\n",
      "=====sample went through ===== 24000000\n",
      "=====sample went through ===== 36000000\n",
      "=====sample went through ===== 48000000\n",
      "time consumed is 957.2496840953827\n",
      "Policy average score =  11.1601771\n",
      "====innerloop: 9 [0, 0, 1, 0, 1, 0, 1, 1, 0] =========\n",
      "=====sample went through ===== 0\n",
      "=====sample went through ===== 12000000\n",
      "=====sample went through ===== 24000000\n",
      "=====sample went through ===== 36000000\n",
      "=====sample went through ===== 48000000\n",
      "time consumed is 865.4016649723053\n",
      "Policy average score =  2.97899146667\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''===========DieN==========='''\n",
    "    '''obs, reward, done , _ = env.step(int(policy[obs]))'''\n",
    "    # isBadSide = [1, 1, 1, 0, 0, 0]\n",
    "#     isBadSide = [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
    "    # isBadSide = [1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0]\n",
    "\n",
    "    # isBadSide = [0,1,1,0]   # 1.4530781083333333\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0,1,0,0]   # 4.695342123333333\n",
    "    # isBadSide = [0,0,0,1]\n",
    "    # isBadSide = [0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1]   # 0.94540250999999997\n",
    "    # isBadSide = [0,1,1,0,0,1,0,0,1,1,1,0,1,0,1,0,1,0,1]\n",
    "    # isBadSide = [0,1,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,1,0,1]      # 8.6673006816666671)\n",
    "    # isBadSide = [0,0,1,1,1,0,0,0,1,0]\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0]\n",
    "    # isBadSide = [0,0,1,0,1,0,1,1,0,1,1,1,1,1,0,1,0]\n",
    "    # isBadSide = [0,0,1,1,0,0,0,0,1,0,1,1,1,0,1,0,1,1]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    isBadSide_list = [\n",
    "                  \n",
    "                   [0,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0],\n",
    "                   [0,1],\n",
    "                   [0,1,0,0,0,0,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,0,0,0,0,1,0,1],\n",
    "                   [0,0,1,0,1,0,1,1,0]\n",
    "                   ]\n",
    "    \n",
    "    for isBad in isBadSide_list:\n",
    "        isBadSide = isBad\n",
    "        n = len(isBadSide)\n",
    "        slip=0\n",
    "        env_DN = DieNEnv(n=n, isBadSide=isBadSide, slip=slip)\n",
    "        env_DN.slip = 0\n",
    "        vi_DN = VI(env_DN)\n",
    "        optimal_policy_DN = vi_DN.optimize(gamma=1)\n",
    "        policy_score = evaluate_policy(env_DN, optimal_policy_DN,\n",
    "#                                        n = 1000\n",
    "                                       n=60000000\n",
    "                                       )\n",
    "        print('Policy average score = ', policy_score)\n",
    "        \n",
    "#     # isBadSide = [0,0,1,1,0,0,0,0,1,0,1,1,1,0,1,0,1,1]   # 4.35475451667\n",
    "#                   \n",
    "#     # isBadSide = [0,0,0,1,1,0,0,0,0,0,1,0,1,0]   # 8.18524228333\n",
    "\n",
    "#     # isBadSide = [0,0,1,1,0,1,1]  # 1.22496245\n",
    "\n",
    "#     # isBadSide = [0,0,0,0,1,1,1,1,1,1,0,0,0,0,1]  # 4.88762186667\n",
    "\n",
    "#     # isBadSide = [0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,1,1,1]  # 8.46418656667\n",
    "\n",
    "#     # isBadSide = [0,1,1,0,0,1,1] # 1.5572027\n",
    "\n",
    "#     # isBadSide = [0,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0] # 8.55319345\n",
    "\n",
    "#     # isBadSide = [0,1] # 0.500020116667\n",
    "\n",
    "#     # isBadSide = [0,1,0,0,0,0,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,0,0,0,0,1,0,1] # 11.1601771\n",
    "\n",
    "#     # isBadSide = [0,0,1,0,1,0,1,1,0]  # 2.97899146667\n",
    "\n",
    "#     n = len(isBadSide)\n",
    "#     slip=0\n",
    "#     env_DN = DieNEnv(n=n, isBadSide=isBadSide, slip=slip)\n",
    "#     env_DN.slip = 0\n",
    "\n",
    "#     # print(env_DN.nA)\n",
    "#     # print(env_DN.nS)\n",
    "#     # print(env_DN.isBadSide)\n",
    "#     # print(\"==========================\")\n",
    "#     # print(env_DN.P)\n",
    "#     # print(\"==========================\")\n",
    "#     vi_DN = VI(env_DN)\n",
    "#     optimal_policy_DN = vi_DN.optimize(gamma=1)\n",
    "\n",
    "#     policy_score = evaluate_policy(env_DN, optimal_policy_DN,\n",
    "#                                    # n = 1000\n",
    "#                                    n=60000000\n",
    "#                                    )\n",
    "#     print('Policy average score = ', policy_score)\n",
    "#     # '''===========Frozenlake==========='''\n",
    "#     # env_name  = 'FrozenLake8x8-v0'\n",
    "#     # env = gym.make(env_name)\n",
    "#     # vi = VI(env)\n",
    "#     # optimal_policy = vi.optimize(gamma=1)\n",
    "#     # policy_score = evaluate_policy(env, optimal_policy, n=1)\n",
    "#     # print('Policy average score = ', policy_score)\n",
    "#     # print('end')\n",
    "#     # '''===========stocks==========='''\n",
    "#     # '''obs, reward, done , _ = env.step(int(policy[obs]))'''\n",
    "#     #\n",
    "#     # env_AT = StocksEnv(df=pd.read_csv('IBM.csv'),frame_bound=(50, 100), window_size=10)\n",
    "#     # print(env_AT.nA)\n",
    "#     # print(env_AT.nS)\n",
    "#     # print(\"==========================\")\n",
    "#     # print(env_AT.P)\n",
    "#     # print(\"==========================\")\n",
    "#     # vi_AT = VI(env_AT)\n",
    "#     # optimal_policy_AT = vi_AT.optimize(gamma=1)\n",
    "#     # policy_score = evaluate_policy_stock(env_AT, optimal_policy_AT, n=1000) \n",
    "#     # print('Policy average score = ', policy_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
