{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from:https://towardsdatascience.com/solving-lunar-lander-openaigym-reinforcement-learning-785675066197\n",
    "\n",
    "# Reinforcement Learning w/ Keras+OpenAI: The Basics\n",
    "\n",
    "Shiva Verma\n",
    "\n",
    "Apr 20, 2019 \n",
    "\n",
    "Lunar Lander is another interesting problem in OpenAIGym. In my previous blog, I solved the classic control environments. In this blog, I will be solving the Lunar Lander environment.\n",
    "## Reinforcement Learning | Brief Intro\n",
    "Reinforcement learning is an interesting area of Machine learning. The rough Idea is that you have an agent and an environment. The agent takes actions and environment gives reward based on those actions, The goal is to teach the agent optimal behaviour in order to maximize the reward received by the environment.\n",
    "\n",
    "## Reinforcement Learning Diagram\n",
    "For example, have a look at the diagram. This maze represents our environment. Our purpose would be to teach the agent an optimal policy so that it can solve this maze. The maze will provide a reward to the agent based on the goodness of each action it takes. Also, each action taken by agent leads it to the new state in the environment.\n",
    "## About Lunar-Lander\n",
    "As you can see in the picture below, there is one space-ship. The task is to land the space-ship between the flags smoothly. The ship has 3 throttles in it. One throttle points downward and other 2 points in the left and right direction. With the help of these, you have to control the Ship.\n",
    "\n",
    "There are 2 different Lunar Lander Environment in OpenAIGym. One has discrete action space and other has continuous action space. Letâ€™s solve both one by one. Please read this doc to know how to use Gym environments.\n",
    "LunarLander-v2 (Discrete)\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "This quote provides enough details about the action and state space. Also, I observed that there is some random wind in the environment which influence the direction of the Ship.\n",
    "## Action space (Discrete)\n",
    "0- Do nothing\n",
    "1- Fire left engine\n",
    "2- Fire down engine\n",
    "3- Fire right engine\n",
    "## Solving the environment\n",
    "I am solving this problem with the DQN algorithm, which is compatible and works well when you have a discrete action space and continuous state space.\n",
    "I will not be going into details of how DQN works. DQN approximate the actions using a neural network. There is much more to read about it. There are pretty good resources on the DQN online. I have included the link of these resources at the end of this blog.\n",
    "## Network Architecture\n",
    "I have attached the snippet of my DQN algorithm which shows network architecture and hyperparameters I have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
